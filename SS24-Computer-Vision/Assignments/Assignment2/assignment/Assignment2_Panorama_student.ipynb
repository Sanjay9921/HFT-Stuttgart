{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panorama Stitching in OpenCV\n",
    "\n",
    "---\n",
    "\n",
    "Dr.-Ing. Antje Muntzinger, Hochschule f√ºr Technik Stuttgart\n",
    "\n",
    "antje.muntzinger@hft-stuttgart.de\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will stitch a panorama from single images in OpenCV. We will explore two methods: a fast method using OpenCV's `Stitcher` class that handles most intermediate steps automatically, and a more hands-on method where we implement the different steps one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install required packages specified in pipfile\n",
    "!pipenv install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Image Preparation\n",
    "=\n",
    "\n",
    "**TODO**: 1a) Load two or more overlapping images that you want to stitch. You can use the provided images, but it is highly encouraged to take some overlapping photos yourself. Note that not all photos can successfully be stitched together without modifications, so in case you encounter problems, try the provided images first. **(1 point)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: adapt n_images to your number of images, and adapt the paths below to load your images\n",
    "# single images named 1.jpg, 2.jpg etc.\n",
    "n_images = 3\n",
    "\n",
    "# fill list of paths \n",
    "image_paths=[]\n",
    "for i in range(n_images):\n",
    "    image_paths.append('images\\\\panorama\\\\'+str(i+1)+'.jpg')\n",
    "    \n",
    "##### END STUDENT CODE\n",
    "    \n",
    "    \n",
    "# fill list of images \n",
    "imgs = [] \n",
    "for i in range(n_images): \n",
    "    img = cv2.imread(image_paths[i])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # in case of memory error, potentially reduce image size\n",
    "    width = int(img.shape[1]/2)\n",
    "    height = int(img.shape[0]/2)\n",
    "    img = cv2.resize(img, (width, height))\n",
    "    \n",
    "    imgs.append(img) \n",
    "     \n",
    "# show the original pictures \n",
    "fig, axes = plt.subplots(1, n_images, figsize=(10, 30))\n",
    "for i in range(n_images): \n",
    "    axes[i].imshow(imgs[i])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panorama stitching - the fast way\n",
    "\n",
    "Let's start by following the fast method to stitch a panorama using OpenCV's `Stitcher` class. Note that depending on your choice of single images, the stitching is not always successful. Also note that due to the underlying random sampling in RANSAC, the result is not deterministic, you might get different results for multiple code runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stitcher object and stitch images\n",
    "stitchy=cv2.Stitcher.create() \n",
    "(dummy,output)=stitchy.stitch(imgs) \n",
    "\n",
    "# check if the stitching procedure was successful   \n",
    "if dummy != cv2.STITCHER_OK: \n",
    "  # .stitch() returns a true value if stitching is  \n",
    "  # done successfully \n",
    "    print(\"Stitching was't successful!\") \n",
    "else:  \n",
    "    print('Your panorama is ready :-)') \n",
    "  \n",
    "    # final output \n",
    "    fig, ax = plt.subplots(1,1, figsize=(10, 30))\n",
    "    ax.imshow(output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panorama stitching - step by step\n",
    "\n",
    "Now let's again stitch a panorama without using the `Stitcher` class in order to understand what is happening under the hood. For the sake of simplicity, we only use the first two images for stitching. First we double the size of the second image to make room for the first image to be warped into this new image. The result looks as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a blank image the same size as the second image - note that an image is just a numpy array here\n",
    "blank_image = np.zeros((imgs[1].shape[0], imgs[1].shape[1], 3), np.uint8)\n",
    "\n",
    "# horizontally concatenates images of same height  \n",
    "imgs[1] = cv2.hconcat([blank_image, imgs[1]]) \n",
    "plt.imshow(imgs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: SIFT Descriptors\n",
    "=\n",
    "\n",
    "Now we use SIFT to detect feature points and descriptors in both images and we plot the descriptors.\n",
    "\n",
    "**TODO**: 2a) Use OpenCV's SIFT method to detect keypoints and descriptors of the two images. Plot the resulting keypoints and descriptors in the two images. **(4 points)**\n",
    "\n",
    "**Hint:** You can use the example code from the lecture slides as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: Instanciate SIFT detector\n",
    "\n",
    "\n",
    "##### TODO: find the keypoints and descriptors with SIFT\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: plot result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 2b) Why do we use SIFT feature points and not Harris Corners for matching? **(2 points)**\n",
    "\n",
    "**YOUR ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Matching Feature Points\n",
    "=\n",
    "\n",
    "**TODO**: 3a) Find matches between the two images using FLANN (Fast Library for Approximate Nearest Neighbors).  **(2 points)**\n",
    "\n",
    "**Hint:** You can find a documentation of FLANN here: https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: find matches with FLANN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 3b) Store good matches following Lowe's ratio test (see lecture slides). Again, you can use the documentation linked above. **(2 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: store all the good matches in a list as per Lowe's ratio test.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 3c) Why do we look at two different distances in Lowe's ratio test? Why not simply use a threshold? **(2 points)**\n",
    "\n",
    "**YOUR ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Model Fitting (RANSAC)\n",
    "=\n",
    "\n",
    "**TODO**: 4a) Use the matches to calculate a homography between the two images. Print the homography matrix. **(3 points)**\n",
    "\n",
    "**Hint**: You can find a tutorial on RANSAC here: https://docs.opencv.org/4.x/d1/de0/tutorial_py_feature_homography.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: YOUR CODE GOES HERE\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** 4b) Which of the 9 values in the homography matrix was not computed by RANSAC? Where does this value come from, and could you theoretically use another number instead?  **(2 points)**\n",
    "\n",
    "**YOUR ANSWER:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 4c) Plot the two images and draw matches between features in the two images in red. Apply the homography to the boundary of the first image and plot the new boundary (after applying the homography) in blue into the second image.  **(4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### TODO: YOUR CODE GOES HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 4d) Finally, warp the first image into the second using the homography. You can use OpenCV's `warpPerspective()` to do this - look up the interface in the online documentation. Afterwards, blend the warped first image and the second image and plot the final panorama. **(2 points)**\n",
    "\n",
    "**Hint:** To blend the two images, you can simply use the `add()` function as learned in the lesson. However, this will cause the overlapping area to be very light, because both pixel values add up. Alternatively, you can use `addWeighted()` with a blending factor `alpha` of 0.5. See the documentation here: https://docs.opencv.org/3.4/d5/dc4/tutorial_adding_images.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### TODO: Warp the first image using the homography\n",
    "\n",
    "\n",
    "##### TODO: Blend the warped image with the second image using alpha blending\n",
    "\n",
    "\n",
    "##### TODO: Display the blended image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You have just stitched your first panorama :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Theory Questions\n",
    "=\n",
    "\n",
    "**TODO**: 5a) You have bought a new camera lens with a focal length of 50mm for your SLR camera. You know that the distance between the lens and the camera sensor is 6cm. How far away from the lens should you place an object to be in focus in the image? Explain your result either by writing the calculations down or by text. **(2.5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 5b) You are using a camera with intrinsic camera matrix $K=\\begin{pmatrix}\n",
    "2642 & 0 & 1034\\\\\n",
    "0 & 2642 & 764\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}$ (values given in pixels). What are the values of your 5 intrinsic parameters? **(2.5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 5c) You are taking a picture of an object located 10 meters away at the point $P=\\begin{pmatrix}0\\\\0\\\\10\\end{pmatrix}$ (in camera coordinates) with the camera from 5b). Which are the pixel coordinates of the corresponding image point? What is special about this point? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: 5d) Consider the image point $\\begin{pmatrix}1034\\\\ 764 \\end{pmatrix}$ in pixel coordinates. What can you tell about the corresponding real object that was projected onto this image point using the camera from 5b)? What do you know about its 3D camera coordinates? **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
